{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxI4LLcnZqRN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "font_path = '../dados/targa/Targa.ttf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SY3XUmVsqEM1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-26 16:44:59.341670: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVhNVxxbZ6qj",
        "outputId": "1ac5b4e0-1aad-4cd1-f1a3-70c23c374ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "unzip:  cannot find or open dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius, dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius.zip or dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ],
      "source": [
        "!wget  -nc https://www.dropbox.com/scl/fi/uaiyxp0t2l8hfcszfadtj/dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius&dl=1\n",
        "!unzip -n -q dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-owKNFD5ah24",
        "outputId": "adecb30f-3918-47dd-f02d-b8c94163f530"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jpg_file</th>\n",
              "      <th>txt_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000001.jpg</td>\n",
              "      <td>RNINIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000002.jpg</td>\n",
              "      <td>TVCFS8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000003.jpg</td>\n",
              "      <td>N1O1EH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000004.jpg</td>\n",
              "      <td>OQZSL4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000005.jpg</td>\n",
              "      <td>GST2YA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     jpg_file txt_content\n",
              "0  000001.jpg      RNINIC\n",
              "1  000002.jpg      TVCFS8\n",
              "2  000003.jpg      N1O1EH\n",
              "3  000004.jpg      OQZSL4\n",
              "4  000005.jpg      GST2YA"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = '../dados/CAPTCHA-10k/treinamento'\n",
        "def generate_df(image_path):\n",
        "  label_path = '../dados/CAPTCHA-10k/labels10k'\n",
        "\n",
        "  jpg_files = [f for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
        "  jpg_files.sort()\n",
        "  data = []\n",
        "\n",
        "  for jpg_file in jpg_files:\n",
        "      txt_file = os.path.splitext(jpg_file)[0] + '.txt'\n",
        "      txt_file_path = os.path.join(label_path, txt_file)\n",
        "\n",
        "      if os.path.exists(txt_file_path):\n",
        "          with open(txt_file_path, 'r') as file:\n",
        "              txt_content = file.read().strip()\n",
        "\n",
        "          data.append({'jpg_file': jpg_file, 'txt_content': txt_content})\n",
        "  return pd.DataFrame(data)\n",
        "\n",
        "df = generate_df(image_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = [ '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
        "num_classes = len(vocab)\n",
        "char_to_index = {char: idx for idx, char in enumerate(vocab)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NGKSD3tFpvIH"
      },
      "outputs": [],
      "source": [
        "def generate_clean_captcha(text):\n",
        "    # Fixed parameters\n",
        "    size = (180, 50)  # Change size to (height, width)\n",
        "    font_size = 24\n",
        "    num_parts = 6\n",
        "\n",
        "    # Create a blank white image\n",
        "    image = Image.new('L', size, 255)  # 'L' mode for grayscale\n",
        "\n",
        "    # Load the custom font\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "    # Create a drawing context\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Calculate positions for each part\n",
        "    part_width = size[0] / num_parts\n",
        "    horizontal_positions = [int(part_width * i + part_width / 2) for i in range(num_parts)]\n",
        "    horizontal_positions = horizontal_positions[:len(text)]  # Adjust to the length of the text\n",
        "\n",
        "    # Calculate y position to center the text vertically\n",
        "    text_bbox = draw.textbbox((0, 0), text, font=font)\n",
        "    text_height = text_bbox[3] - text_bbox[1]\n",
        "    text_y = (size[1] - text_height) // 2\n",
        "\n",
        "    # Draw each letter at the calculated position\n",
        "    for char, x in zip(text, horizontal_positions):\n",
        "        char_bbox = draw.textbbox((0, 0), char, font=font)\n",
        "        char_width = char_bbox[2] - char_bbox[0]\n",
        "        char_x = x - char_width // 2  # Center the character horizontally within its part\n",
        "        draw.text((char_x, text_y), char, font=font, fill=0)\n",
        "\n",
        "    # Convert to numpy array if needed for further processing with OpenCV\n",
        "    captcha_image = np.array(image)\n",
        "\n",
        "    return captcha_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "  kernel  = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
        "  img     = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "  _, img  = cv2.threshold(img, 90, 255, cv2.THRESH_BINARY)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zpd9GwIhpKMe"
      },
      "outputs": [],
      "source": [
        "def generate_X_Y(image_path):\n",
        "  df = generate_df(image_path)\n",
        "  X = [preprocess(cv2.imread(os.path.join(image_path, x),cv2.IMREAD_GRAYSCALE)) for x in df[\"jpg_file\"]]\n",
        "  X = np.array(X)\n",
        "  X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "  Y = np.array([generate_clean_captcha(x[:6]) for x in df[\"txt_content\"]])\n",
        "  X = X.astype('float32') / 255.\n",
        "  Y = Y.astype('float32') / 255.\n",
        "\n",
        "  \n",
        "  return X,Y,df['txt_content']\n",
        "\n",
        "X_train, Y_train,labels_train = generate_X_Y('../dados/CAPTCHA-10k/treinamento')\n",
        "X_val, Y_val,labels_val = generate_X_Y('../dados/CAPTCHA-10k/validacao')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-26 16:45:05.856219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13795 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "def build_dataset(X,predictions, labels, batch_size=32):\n",
        "    def encode_labels(labels):\n",
        "        # Create an array to store one-hot encoded labels\n",
        "        encoded_labels = np.zeros((len(labels), 6, num_classes), dtype=np.float32)\n",
        "        \n",
        "        for i, label in enumerate(labels):\n",
        "            for j, char in enumerate(label[:6]):\n",
        "                index = char_to_index.get(char, -1)\n",
        "                encoded_labels[i, j, index] = 1.0\n",
        "        return encoded_labels\n",
        "    \n",
        "    Y = encode_labels(labels)\n",
        "    \n",
        "    # Create TensorFlow datasets from X and Y\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, predictions,Y))\n",
        "\n",
        "    # Shuffle, batch, and prefetch the dataset\n",
        "    buffer_size = len(X)  # Typically set to the size of the dataset\n",
        "    dataset = dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return dataset\n",
        "train_dataset = build_dataset(X_train,Y_train, labels_train)\n",
        "val_dataset = build_dataset(X_val,Y_val, labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    max_pixel = 1.0\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=max_pixel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "autoencoder = tf.keras.models.load_model('model_BCE_aug_best_unet.tf',custom_objects={\"rmse\": rmse,\"psnr\":psnr})\n",
        "classifier = tf.keras.models.load_model('classifier_pre_trained.tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classification_loss(y_true, y_pred):\n",
        "    # Reshape y_true and y_pred to match the shape expected for loss calculation\n",
        "    return tf.reduce_mean(tf.keras.losses.CategoricalCrossentropy()(y_true, y_pred))\n",
        "\n",
        "def reconstruction_loss(autoencoder_output, captcha_predictions):\n",
        "    return tf.keras.losses.binary_crossentropy(tf.keras.backend.flatten(autoencoder_output), tf.keras.backend.flatten(captcha_predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "LAMBDA = 10\n",
        "@tf.function\n",
        "def train_step(input_images, true_images, labels, autoencoder, classifier, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass through the autoencoder\n",
        "        autoencoder_output  = autoencoder(input_images, training=True)\n",
        "        predictions_list = []\n",
        "\n",
        "        c_loss = 0.0\n",
        "        interval = [0,30,60,90,120,150,180]\n",
        "        for i in range(1,len(interval)):\n",
        "            fake_img = autoencoder_output[:,:,interval[i-1]:interval[i],:]\n",
        "            y_pred = classifier(fake_img, training=True)\n",
        "\n",
        "            predictions_list.append(y_pred)\n",
        "            c_loss += classification_loss(labels[:,i-1], y_pred)\n",
        "\n",
        "        c_loss /= len(interval)\n",
        "        r_loss = reconstruction_loss(autoencoder_output, true_images)\n",
        "        loss = r_loss + LAMBDA*c_loss\n",
        "        \n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, autoencoder.trainable_variables + classifier.trainable_variables)\n",
        "    \n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables + classifier.trainable_variables))\n",
        "\n",
        "    #  Compute accuracy\n",
        "    predictions = tf.concat(predictions_list, axis=0)  # Concatenate predictions from all patches\n",
        "    predicted_labels = tf.argmax(predictions, axis=-1)  # Convert to class indices\n",
        "    predicted_labels = tf.reshape(predicted_labels,[-1,6])\n",
        "    true_labels = tf.argmax(labels, axis=-1)  # Convert true labels to class indices\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, true_labels), tf.float32))\n",
        "    return loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-26 16:45:13.059949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
            "2024-07-26 16:45:14.003791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2024-07-26 16:45:14.179541: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7be34bcdacc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-07-26 16:45:14.179564: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
            "2024-07-26 16:45:14.183796: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-07-26 16:45:14.295525: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 14.763928413391113  acc 0.0572916679084301\n",
            "Training loss: 7.739585876464844  acc 0.02083333395421505\n",
            "Training loss: 8.827205657958984  acc 0.0520833320915699\n",
            "Training loss: 6.311063289642334  acc 0.03125\n",
            "Training loss: 5.5601630210876465  acc 0.0572916679084301\n",
            "Training loss: 3.7353014945983887  acc 0.02083333395421505\n",
            "Training loss: 3.5528697967529297  acc 0.0625\n",
            "Training loss: 4.06256628036499  acc 0.03125\n",
            "Training loss: 4.330389499664307  acc 0.046875\n",
            "Training loss: 2.0340499877929688  acc 0.03125\n",
            "Training loss: 2.915771961212158  acc 0.0520833320915699\n",
            "Training loss: 2.887209892272949  acc 0.046875\n",
            "Training loss: 3.3509342670440674  acc 0.02083333395421505\n",
            "Training loss: 1.7997829914093018  acc 0.03125\n",
            "Training loss: 1.803154468536377  acc 0.0625\n",
            "Training loss: 2.359067440032959  acc 0.02604166604578495\n",
            "Training loss: 2.098780393600464  acc 0.02604166604578495\n",
            "Training loss: 1.812424898147583  acc 0.0520833320915699\n",
            "Training loss: 2.8251683712005615  acc 0.046875\n",
            "Training loss: 2.658015012741089  acc 0.0364583320915699\n",
            "Training loss: 3.605454921722412  acc 0.02083333395421505\n",
            "Training loss: 2.5838546752929688  acc 0.03125\n",
            "Training loss: 2.656421422958374  acc 0.015625\n",
            "Training loss: 1.559758186340332  acc 0.0520833320915699\n",
            "Training loss: 2.475494146347046  acc 0.0364583320915699\n",
            "Training loss: 1.3082042932510376  acc 0.0364583320915699\n",
            "Training loss: 2.4948275089263916  acc 0.02604166604578495\n",
            "Training loss: 1.4265813827514648  acc 0.02604166604578495\n",
            "Training loss: 1.9020055532455444  acc 0.0520833320915699\n",
            "Training loss: 1.460181713104248  acc 0.02604166604578495\n",
            "Training loss: 2.978050947189331  acc 0.0625\n",
            "Training loss: 2.983147621154785  acc 0.0416666679084301\n",
            "Training loss: 2.598283052444458  acc 0.03125\n",
            "Training loss: 3.6836278438568115  acc 0.0520833320915699\n",
            "Training loss: 1.7380716800689697  acc 0.0364583320915699\n",
            "Training loss: 2.0006561279296875  acc 0.0572916679084301\n",
            "Training loss: 2.298438549041748  acc 0.046875\n",
            "Training loss: 3.782970905303955  acc 0.02083333395421505\n",
            "Training loss: 1.7986395359039307  acc 0.0364583320915699\n",
            "Training loss: 2.332228183746338  acc 0.0416666679084301\n",
            "Training loss: 2.4654150009155273  acc 0.02604166604578495\n",
            "Training loss: 1.9198384284973145  acc 0.02604166604578495\n",
            "Training loss: 2.5277392864227295  acc 0.03125\n",
            "Training loss: 3.1910958290100098  acc 0.03125\n",
            "Training loss: 1.742754578590393  acc 0.0364583320915699\n",
            "Training loss: 2.0309271812438965  acc 0.0364583320915699\n",
            "Training loss: 3.5593621730804443  acc 0.03125\n",
            "Training loss: 3.935455799102783  acc 0.0364583320915699\n",
            "Training loss: 1.9840158224105835  acc 0.02604166604578495\n",
            "Training loss: 3.1480648517608643  acc 0.03125\n",
            "Training loss: 3.263827323913574  acc 0.02604166604578495\n",
            "Training loss: 1.9040862321853638  acc 0.0416666679084301\n",
            "Training loss: 1.7904667854309082  acc 0.0364583320915699\n",
            "Training loss: 1.8335446119308472  acc 0.02604166604578495\n",
            "Training loss: 1.500809907913208  acc 0.015625\n",
            "Training loss: 1.980729341506958  acc 0.046875\n",
            "Training loss: 2.77763032913208  acc 0.0416666679084301\n",
            "Training loss: 2.470907211303711  acc 0.02604166604578495\n",
            "Training loss: 2.6135122776031494  acc 0.02604166604578495\n",
            "Training loss: 1.5243148803710938  acc 0.02604166604578495\n",
            "Training loss: 2.0401113033294678  acc 0.03125\n",
            "Training loss: 1.5501354932785034  acc 0.046875\n",
            "Training loss: 4.419737815856934  acc 0.0364583320915699\n",
            "Training loss: 1.4717276096343994  acc 0.02604166604578495\n",
            "Training loss: 1.8476802110671997  acc 0.0520833320915699\n",
            "Training loss: 3.073233127593994  acc 0.0416666679084301\n",
            "Training loss: 1.98262619972229  acc 0.0364583320915699\n",
            "Training loss: 2.1664555072784424  acc 0.02083333395421505\n",
            "Training loss: 2.2600607872009277  acc 0.02604166604578495\n",
            "Training loss: 2.5183725357055664  acc 0.02083333395421505\n",
            "Training loss: 2.265970230102539  acc 0.02083333395421505\n",
            "Training loss: 1.8459599018096924  acc 0.0364583320915699\n",
            "Training loss: 1.7030779123306274  acc 0.03125\n",
            "Training loss: 1.258440613746643  acc 0.0416666679084301\n",
            "Training loss: 1.4114900827407837  acc 0.0520833320915699\n",
            "Training loss: 1.0455918312072754  acc 0.0625\n",
            "Training loss: 1.588886022567749  acc 0.0520833320915699\n",
            "Training loss: 1.055453896522522  acc 0.0364583320915699\n",
            "Training loss: 1.5812888145446777  acc 0.02604166604578495\n",
            "Training loss: 1.2324022054672241  acc 0.03125\n",
            "Training loss: 1.681999683380127  acc 0.03125\n",
            "Training loss: 1.2164307832717896  acc 0.02083333395421505\n",
            "Training loss: 1.8825172185897827  acc 0.0364583320915699\n",
            "Training loss: 1.371935248374939  acc 0.0520833320915699\n",
            "Training loss: 1.413475513458252  acc 0.0364583320915699\n",
            "Training loss: 1.844081163406372  acc 0.0625\n",
            "Training loss: 2.0465800762176514  acc 0.03125\n",
            "Training loss: 1.8025622367858887  acc 0.0364583320915699\n",
            "Training loss: 2.487119197845459  acc 0.0364583320915699\n",
            "Training loss: 3.032456874847412  acc 0.010416666977107525\n",
            "Training loss: 1.9581807851791382  acc 0.0416666679084301\n",
            "Training loss: 1.6661529541015625  acc 0.0416666679084301\n",
            "Training loss: 2.370903730392456  acc 0.0416666679084301\n",
            "Training loss: 2.28752064704895  acc 0.046875\n",
            "Training loss: 2.283756732940674  acc 0.03125\n",
            "Training loss: 3.194664716720581  acc 0.0520833320915699\n",
            "Training loss: 1.902240514755249  acc 0.0364583320915699\n",
            "Training loss: 1.9421833753585815  acc 0.0364583320915699\n",
            "Training loss: 2.0397751331329346  acc 0.0052083334885537624\n",
            "Training loss: 1.1375809907913208  acc 0.0364583320915699\n",
            "Training loss: 1.9575839042663574  acc 0.03125\n",
            "Training loss: 2.1315791606903076  acc 0.0833333358168602\n",
            "Training loss: 2.792133331298828  acc 0.0364583320915699\n",
            "Training loss: 1.7684005498886108  acc 0.0677083358168602\n",
            "Training loss: 3.581888198852539  acc 0.02604166604578495\n",
            "Training loss: 1.9381533861160278  acc 0.0416666679084301\n",
            "Training loss: 1.808412790298462  acc 0.015625\n",
            "Training loss: 1.308354377746582  acc 0.046875\n",
            "Training loss: 1.5610220432281494  acc 0.0416666679084301\n",
            "Training loss: 1.9826247692108154  acc 0.02083333395421505\n",
            "Training loss: 1.9429408311843872  acc 0.03125\n",
            "Training loss: 2.3433713912963867  acc 0.0364583320915699\n",
            "Training loss: 2.2807652950286865  acc 0.0520833320915699\n",
            "Training loss: 2.0992431640625  acc 0.0520833320915699\n",
            "Training loss: 1.917924165725708  acc 0.0364583320915699\n",
            "Training loss: 2.892510175704956  acc 0.02604166604578495\n",
            "Training loss: 2.499683380126953  acc 0.0572916679084301\n",
            "Training loss: 2.2448487281799316  acc 0.02604166604578495\n",
            "Training loss: 1.7981233596801758  acc 0.03125\n",
            "Training loss: 1.9757670164108276  acc 0.02604166604578495\n",
            "Training loss: 3.0771679878234863  acc 0.03125\n",
            "Training loss: 1.6782314777374268  acc 0.03125\n",
            "Training loss: 1.8605378866195679  acc 0.046875\n",
            "Training loss: 2.7033867835998535  acc 0.0416666679084301\n",
            "Training loss: 2.015465259552002  acc 0.02083333395421505\n",
            "Training loss: 2.5756335258483887  acc 0.03125\n",
            "Training loss: 1.4277558326721191  acc 0.02083333395421505\n",
            "Training loss: 1.4247047901153564  acc 0.03125\n",
            "Training loss: 1.3278915882110596  acc 0.03125\n",
            "Training loss: 1.049188494682312  acc 0.0625\n",
            "Training loss: 2.6544833183288574  acc 0.02604166604578495\n",
            "Training loss: 2.2325429916381836  acc 0.046875\n",
            "Training loss: 1.2726333141326904  acc 0.03125\n",
            "Training loss: 1.9234764575958252  acc 0.03125\n",
            "Training loss: 4.311066150665283  acc 0.02604166604578495\n",
            "Training loss: 1.3886526823043823  acc 0.02604166604578495\n",
            "Training loss: 2.205315351486206  acc 0.046875\n",
            "Training loss: 1.9810974597930908  acc 0.02604166604578495\n",
            "Training loss: 1.2389472723007202  acc 0.02083333395421505\n",
            "Training loss: 1.0639948844909668  acc 0.03125\n",
            "Training loss: 2.0629448890686035  acc 0.0364583320915699\n",
            "Training loss: 2.072355270385742  acc 0.015625\n",
            "Training loss: 1.0924594402313232  acc 0.03125\n",
            "Training loss: 2.160059690475464  acc 0.0416666679084301\n",
            "Training loss: 1.6480549573898315  acc 0.0364583320915699\n",
            "Training loss: 0.9666074514389038  acc 0.0364583320915699\n",
            "Training loss: 1.8432533740997314  acc 0.0364583320915699\n",
            "Training loss: 0.864836573600769  acc 0.0416666679084301\n",
            "Training loss: 1.650636076927185  acc 0.0364583320915699\n",
            "Training loss: 2.154325485229492  acc 0.03125\n",
            "Training loss: 1.8731917142868042  acc 0.02604166604578495\n",
            "Training loss: 1.2992545366287231  acc 0.03125\n",
            "Training loss: 2.7326552867889404  acc 0.046875\n",
            "Training loss: 0.9716942310333252  acc 0.02604166604578495\n",
            "Training loss: 1.5205014944076538  acc 0.046875\n",
            "Training loss: 1.3216687440872192  acc 0.0677083358168602\n",
            "Training loss: 1.7419260740280151  acc 0.046875\n",
            "Training loss: 1.498976230621338  acc 0.0364583320915699\n",
            "Training loss: 1.6100473403930664  acc 0.0364583320915699\n",
            "Training loss: 1.762362003326416  acc 0.0572916679084301\n",
            "Training loss: 2.24605655670166  acc 0.03125\n",
            "Training loss: 1.4785258769989014  acc 0.010416666977107525\n",
            "Training loss: 1.5676977634429932  acc 0.0416666679084301\n",
            "Training loss: 1.5624555349349976  acc 0.046875\n",
            "Training loss: 2.059129238128662  acc 0.0625\n",
            "Training loss: 1.7827587127685547  acc 0.02083333395421505\n",
            "Training loss: 1.6632496118545532  acc 0.046875\n",
            "Training loss: 1.000277042388916  acc 0.02604166604578495\n",
            "Training loss: 1.681188941001892  acc 0.03125\n",
            "Training loss: 1.754948377609253  acc 0.02083333395421505\n",
            "Training loss: 2.3674986362457275  acc 0.03125\n",
            "Training loss: 2.03257417678833  acc 0.0364583320915699\n",
            "Training loss: 1.6710671186447144  acc 0.0625\n",
            "Training loss: 1.3110929727554321  acc 0.0364583320915699\n",
            "Training loss: 1.202735185623169  acc 0.02604166604578495\n",
            "Training loss: 1.214080572128296  acc 0.0416666679084301\n",
            "Training loss: 1.0664019584655762  acc 0.03125\n",
            "Training loss: 1.6059825420379639  acc 0.046875\n",
            "Training loss: 1.0784728527069092  acc 0.0364583320915699\n",
            "Training loss: 2.1238741874694824  acc 0.078125\n",
            "Training loss: 4.348092555999756  acc 0.0416666679084301\n",
            "Training loss: 2.700255870819092  acc 0.0416666679084301\n",
            "Training loss: 1.3459405899047852  acc 0.046875\n",
            "Training loss: 1.9711239337921143  acc 0.02083333395421505\n",
            "Training loss: 1.0563842058181763  acc 0.0416666679084301\n",
            "Training loss: 2.96951961517334  acc 0.0364583320915699\n",
            "Training loss: 1.2964636087417603  acc 0.0520833320915699\n",
            "Training loss: 2.142808437347412  acc 0.03125\n",
            "Training loss: 1.009621024131775  acc 0.046875\n",
            "Training loss: 2.2784924507141113  acc 0.0364583320915699\n",
            "Training loss: 2.2113428115844727  acc 0.046875\n",
            "Training loss: 1.4773908853530884  acc 0.03125\n",
            "Training loss: 2.0548620223999023  acc 0.046875\n",
            "Training loss: 0.8814456462860107  acc 0.0520833320915699\n",
            "Training loss: 1.626052737236023  acc 0.046875\n",
            "Training loss: 1.219101071357727  acc 0.0416666679084301\n",
            "Training loss: 1.639679193496704  acc 0.0364583320915699\n",
            "Training loss: 1.3307932615280151  acc 0.0520833320915699\n",
            "Training loss: 1.5087333917617798  acc 0.03125\n",
            "Training loss: 1.8678758144378662  acc 0.046875\n",
            "Training loss: 1.2535364627838135  acc 0.03125\n",
            "Training loss: 1.988305687904358  acc 0.0364583320915699\n",
            "Training loss: 1.7189806699752808  acc 0.02604166604578495\n",
            "Training loss: 1.5235852003097534  acc 0.0364583320915699\n",
            "Training loss: 1.3414057493209839  acc 0.02083333395421505\n",
            "Training loss: 2.3818423748016357  acc 0.0364583320915699\n",
            "Training loss: 1.1477583646774292  acc 0.02604166604578495\n",
            "Training loss: 1.0174983739852905  acc 0.046875\n",
            "Training loss: 2.1662702560424805  acc 0.046875\n",
            "Training loss: 1.2640843391418457  acc 0.0364583320915699\n",
            "Training loss: 1.778252363204956  acc 0.02083333395421505\n",
            "Training loss: 1.5206873416900635  acc 0.03125\n",
            "Training loss: 2.346011161804199  acc 0.02604166604578495\n",
            "Training loss: 0.9347165822982788  acc 0.0572916679084301\n",
            "Training loss: 1.8869928121566772  acc 0.0416666679084301\n",
            "Training loss: 1.7790613174438477  acc 0.0416666679084301\n",
            "Training loss: 1.3311883211135864  acc 0.0520833320915699\n",
            "Training loss: 1.3350645303726196  acc 0.03125\n",
            "Training loss: 1.7354120016098022  acc 0.0625\n",
            "Training loss: 1.0291467905044556  acc 0.0364583320915699\n",
            "Training loss: 1.1272891759872437  acc 0.0520833320915699\n",
            "Training loss: 2.2784507274627686  acc 0.0572916679084301\n",
            "Training loss: 1.6793463230133057  acc 0.02083333395421505\n",
            "Training loss: 2.205540657043457  acc 0.015625\n",
            "Training loss: 1.2653307914733887  acc 0.046875\n",
            "Training loss: 1.3754913806915283  acc 0.0416666679084301\n",
            "Training loss: 0.8831580877304077  acc 0.03125\n",
            "Training loss: 2.6264190673828125  acc 0.0520833320915699\n",
            "Training loss: 2.229994535446167  acc 0.0572916679084301\n",
            "Training loss: 1.5328270196914673  acc 0.0364583320915699\n",
            "Training loss: 1.2548236846923828  acc 0.03125\n",
            "Training loss: 2.241159677505493  acc 0.0416666679084301\n",
            "Training loss: 1.8350862264633179  acc 0.046875\n",
            "Training loss: 1.2374353408813477  acc 0.046875\n",
            "Training loss: 1.0715631246566772  acc 0.0416666679084301\n",
            "Training loss: 1.8646512031555176  acc 0.0364583320915699\n",
            "Training loss: 1.647392988204956  acc 0.02083333395421505\n",
            "Training loss: 1.509243130683899  acc 0.02083333395421505\n",
            "Training loss: 2.487159252166748  acc 0.03125\n",
            "Training loss: 1.7868603467941284  acc 0.0364583320915699\n",
            "Training loss: 0.9713429808616638  acc 0.02604166604578495\n",
            "Training loss: 2.0057005882263184  acc 0.0416666679084301\n",
            "Training loss: 1.5502324104309082  acc 0.0364583320915699\n",
            "Training loss: 1.252055048942566  acc 0.046875\n",
            "Training loss: 1.873816967010498  acc 0.03125\n",
            "Training loss: 1.936843991279602  acc 0.0416666679084301\n",
            "Training loss: 1.4464815855026245  acc 0.02083333395421505\n",
            "Training loss: 1.4979814291000366  acc 0.02604166604578495\n",
            "Training loss: 2.0348448753356934  acc 0.0729166641831398\n",
            "Training loss: 1.3212071657180786  acc 0.046875\n"
          ]
        }
      ],
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "epochs = 1\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    \n",
        "    # Training loop\n",
        "    for (input_images,true_images, labels) in train_dataset:\n",
        "        loss,acc =  train_step(input_images, true_images, labels, autoencoder, classifier, optimizer)\n",
        "        print(f\"Training loss: {loss.numpy()}  acc {acc.numpy()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
