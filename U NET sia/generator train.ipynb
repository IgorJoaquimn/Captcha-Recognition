{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxI4LLcnZqRN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SY3XUmVsqEM1"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Input,Flatten,Reshape,AveragePooling2D,Dropout,LayerNormalization, ReLU,concatenate,Cropping2D, BatchNormalization\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input,Dropout, ReLU,BatchNormalization,Concatenate,LeakyReLU,Identity\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVhNVxxbZ6qj",
        "outputId": "1ac5b4e0-1aad-4cd1-f1a3-70c23c374ef6"
      },
      "outputs": [],
      "source": [
        "!wget  -nc https://www.dropbox.com/scl/fi/uaiyxp0t2l8hfcszfadtj/dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius&dl=1\n",
        "!unzip -n -q dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-owKNFD5ah24",
        "outputId": "adecb30f-3918-47dd-f02d-b8c94163f530"
      },
      "outputs": [],
      "source": [
        "image_path = '../dados/CAPTCHA-10k/treinamento'\n",
        "def generate_df(image_path):\n",
        "  label_path = '../dados/CAPTCHA-10k/labels10k'\n",
        "\n",
        "  jpg_files = [f for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
        "  jpg_files.sort()\n",
        "  data = []\n",
        "\n",
        "  for jpg_file in jpg_files:\n",
        "      txt_file = os.path.splitext(jpg_file)[0] + '.txt'\n",
        "      txt_file_path = os.path.join(label_path, txt_file)\n",
        "\n",
        "      if os.path.exists(txt_file_path):\n",
        "          with open(txt_file_path, 'r') as file:\n",
        "              txt_content = file.read().strip()\n",
        "\n",
        "          data.append({'jpg_file': jpg_file, 'txt_content': txt_content})\n",
        "  return pd.DataFrame(data)\n",
        "\n",
        "df = generate_df(image_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOjGMcBJpcxt",
        "outputId": "abbf5544-6fdf-4684-ae99-cf322310b255"
      },
      "outputs": [],
      "source": [
        "df['txt_content'].str.len().value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "font_path = '../dados/targa/Targa.ttf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGKSD3tFpvIH"
      },
      "outputs": [],
      "source": [
        "def generate_clean_captcha(text):\n",
        "    # Fixed parameters\n",
        "    size = (180, 50)  # Change size to (height, width)\n",
        "    font_size = 24\n",
        "    num_parts = 6\n",
        "\n",
        "    # Create a blank white image\n",
        "    image = Image.new('L', size, 255)  # 'L' mode for grayscale\n",
        "\n",
        "    # Load the custom font\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "    # Create a drawing context\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Calculate positions for each part\n",
        "    part_width = size[0] / num_parts\n",
        "    horizontal_positions = [int(part_width * i + part_width / 2) for i in range(num_parts)]\n",
        "    horizontal_positions = horizontal_positions[:len(text)]  # Adjust to the length of the text\n",
        "\n",
        "    # Calculate y position to center the text vertically\n",
        "    text_bbox = draw.textbbox((0, 0), text, font=font)\n",
        "    text_height = text_bbox[3] - text_bbox[1]\n",
        "    text_y = (size[1] - text_height) // 2\n",
        "\n",
        "    # Draw each letter at the calculated position\n",
        "    for char, x in zip(text, horizontal_positions):\n",
        "        char_bbox = draw.textbbox((0, 0), char, font=font)\n",
        "        char_width = char_bbox[2] - char_bbox[0]\n",
        "        char_x = x - char_width // 2  # Center the character horizontally within its part\n",
        "        draw.text((char_x, text_y), char, font=font, fill=0)\n",
        "\n",
        "    # Convert to numpy array if needed for further processing with OpenCV\n",
        "    captcha_image = np.array(image)\n",
        "\n",
        "    return captcha_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "  kernel  = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
        "  img     = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "  _, img  = cv2.threshold(img, 90, 255, cv2.THRESH_BINARY)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpd9GwIhpKMe"
      },
      "outputs": [],
      "source": [
        "def generate_X_Y(image_path):\n",
        "  df = generate_df(image_path)\n",
        "  X = [preprocess(cv2.imread(os.path.join(image_path, x),cv2.IMREAD_GRAYSCALE)) for x in df[\"jpg_file\"]]\n",
        "  X = np.array(X)\n",
        "  X = np.expand_dims(X, axis=-1)\n",
        "\n",
        "  Y = np.array([generate_clean_captcha(x[:6]) for x in df[\"txt_content\"]])\n",
        "  X = X.astype('float32') / 255.\n",
        "  Y = (Y.astype('float32') - 127.5)  / 127.5\n",
        "  return X,Y\n",
        "\n",
        "X_train, Y_train = generate_X_Y('../dados/CAPTCHA-10k/treinamento')\n",
        "X_val, Y_val = generate_X_Y('../dados/CAPTCHA-10k/validacao')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "QBnc_yZxb-at",
        "outputId": "7287c59d-c697-4fbf-eae5-8b6d6b6adf26"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(X_train[0], cmap='gray')\n",
        "\n",
        "# Display the corresponding Y_train[0]\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(Y_train[0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmnJm6_VxcPe"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X_train)\n",
        "batch_size = 64\n",
        "augmented_data_generator = datagen.flow(X_train, Y_train, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "ZN3aZqlGxfiq",
        "outputId": "a005973a-e4f7-485b-8f29-22279b015e83"
      },
      "outputs": [],
      "source": [
        "augmented_images, augmented_labels = next(augmented_data_generator)\n",
        "\n",
        "# Display the first few augmented images and their corresponding labels\n",
        "plt.figure(figsize=(9, 9))\n",
        "for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[i].reshape(50, 180), cmap='gray')  # Assuming images are grayscale\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3EVU-sQfUxs"
      },
      "outputs": [],
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    max_pixel = 1.0\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=max_pixel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Camada Convolution-BatchNorm-ReLu\n",
        "def CK(filters, kernel_size=(4, 4), strides=(2, 2), padding='same', use_batch_norm=True, downsample=True):\n",
        "    '''\n",
        "        filters: quantidade de filtros\n",
        "        kernel_size 3x3 | strides 1x1 | padding same | sao constantes durante o codigo\n",
        "        use_batch_norm ->   indica quando devemos usar BatchNormalization, em caso de negativo, a camada se torna a Identidade\n",
        "        downsample ->       indica se a dimensao deve aumentar ou diminuir \n",
        "    '''\n",
        "\n",
        "    # Esse chavemento usando  if ternario serve para selecionar as camadas com base nos atributos\n",
        "    conv = Conv2D               if downsample       else Conv2DTranspose\n",
        "    norm = BatchNormalization   if use_batch_norm   else Identity\n",
        "    actf = LeakyReLU(0.2)       if downsample       else ReLU()\n",
        "    # alpha de 0.2 na LeakyReLU foi definido no paper original\n",
        "\n",
        "    # Com o chaveamento pronto, a camada pode ser montada sequencialmente\n",
        "    def layer(x):\n",
        "        x = conv(filters, kernel_size, strides=strides, padding=padding)(x)\n",
        "        x = norm()(x)\n",
        "        x = actf(x)\n",
        "        return x\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crop_and_concat(x1, x2):\n",
        "    # Crop x2 to the shape of x1\n",
        "    x1_shape = x1.shape\n",
        "    x2_shape = x2.shape\n",
        "    height_diff = x2_shape[1] - x1_shape[1]\n",
        "    width_diff = x2_shape[2] - x1_shape[2]\n",
        "\n",
        "    cropping = ((height_diff // 2, height_diff - height_diff // 2),\n",
        "                (width_diff // 2, width_diff - width_diff // 2))\n",
        "    x2_cropped = Cropping2D(cropping)(x2)\n",
        "    return concatenate([x1, x2_cropped])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generator(input_nc, output_nc, ngf, num_blocks=1, num_downsample=3):\n",
        "    # Aqui, definimos o modelo gerador\n",
        "    \n",
        "    # Ele deve receber uma imagem (img_size,img_size,input_nc)\n",
        "    inputs = Input(shape=(50, 180, input_nc))\n",
        "\n",
        "    # --------------- START ENCODER --------------------------\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    # Uma convolucao eh aplicada inicialmente sem o batch_norm\n",
        "    x = CK(ngf,use_batch_norm=False)(x)\n",
        "    # img_shape = img_size/2,img_size/2,ngf\n",
        "\n",
        "    # Aqui implementamos o caminho onde a dimensao da imagem vai diminuindo\n",
        "    #Armazenamos as skip connections\n",
        "    skips = []\n",
        "    for i in range(num_downsample + num_blocks - 1):\n",
        "        # Como o numero de filtros vai aumentando exponencialmente, existe um momento em que o crescimento eh cessado e a qtd se torna constante\n",
        "        # Este e o momento em que i+1 >= num_downsample\n",
        "        # A partir dai, o numero de filtros nao diminui\n",
        "        expo = min(i+1,num_downsample)\n",
        "        # Aplicamos convolucao com o numero de filtros escolhido\n",
        "        x = CK(ngf*(2**(expo)))(x)\n",
        "        # Guardamos a coneccao\n",
        "        skips.append(x)\n",
        "    \n",
        "    # --------------- FIM   ENCODER --------------------------\n",
        "    '''\n",
        "    Aqui, a imagem esta no formato:\n",
        "    img_shape = img_size/(num_downsample + num_blocks),img_size/(num_downsample + num_blocks),ngf*(2**(expo))\n",
        "    \n",
        "    '''\n",
        "    # --------------- START DECODER --------------------------\n",
        "    # As conexoes sao processadas de tras pra frente, ja que a ultima skip gerada vai se ligar com a primeira camada do decoder\n",
        "    skips = list(reversed(skips))\n",
        "\n",
        "\n",
        "    # Para as skips que sobraram, devemos conecta-las a camadas sem dropout\n",
        "    for skip in skips:\n",
        "        x = crop_and_concat(skip,x)\n",
        "        # x = Concatenate()([x, skip])\n",
        "        x = CK(ngf * (2 ** i),downsample=False)(x)\n",
        "    \n",
        "    #    img_shape = img_size/(2),img_size/(2),ngf*(2**(expo))\n",
        "    # --------------- FIM   DECODER --------------------------\n",
        "\n",
        "    # Por fim, uma ultima convolucao com TanH para gerar o resultado final\n",
        "    # Numero de canais = output_nc  = 3 = RGB\n",
        "    x = Conv2DTranspose(ngf, (4, 4),strides=(2,2), activation='relu',padding=\"same\")(x)\n",
        "    x = Conv2D(ngf, (3, 1),strides=(1,1), activation='relu')(x)\n",
        "    output = Conv2D(output_nc, (1, 1),strides=(1,1), activation='tanh')(x)\n",
        "    return Model(inputs=inputs, outputs=output)\n",
        "\n",
        "autoencoder = generator(1, 1, 32, num_blocks=3, num_downsample=4)\n",
        "autoencoder.compile(optimizer=Adam(2e-4, beta_1=0.5,beta_2=0.999), loss=MeanSquaredError(),metrics=[rmse,psnr])\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEeK5i-XosYB"
      },
      "outputs": [],
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'model_MSE_aug_best_unet.tf',\n",
        "    monitor='val_rmse',\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-ky9n_4VO2g",
        "outputId": "4dc87997-1d28-4a42-c4b1-470ad68ffb87"
      },
      "outputs": [],
      "source": [
        "history = autoencoder.fit(augmented_data_generator,\n",
        "                steps_per_epoch=len(X_train) // batch_size,\n",
        "                epochs=300,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                callbacks=[checkpoint],\n",
        "                validation_data=(X_val, Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bycR9-wTXadc",
        "outputId": "0e31fc0a-d09e-469e-f618-f65186cfea00"
      },
      "outputs": [],
      "source": [
        "Y_train_pred = autoencoder.predict(X_train)\n",
        "Y_val_pred = autoencoder.predict(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMmkDR-ceAXk"
      },
      "outputs": [],
      "source": [
        "def display_images(images, Y, Y_pred, num_images=10):\n",
        "    indices = np.random.choice(len(images), num_images, replace=False)\n",
        "    plt.figure(figsize=(20, 6))\n",
        "    for i, idx in enumerate(indices):\n",
        "        # Original images\n",
        "        ax = plt.subplot(3, num_images, i + 1)\n",
        "        plt.imshow(images[idx].reshape(50,180), cmap=\"gray\")\n",
        "        plt.title(\"Original\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Ground truth labels (Y)\n",
        "        ax = plt.subplot(3, num_images, i + 1 + num_images)\n",
        "        plt.imshow(Y[idx].reshape(50, 180), cmap=\"gray\")\n",
        "        plt.title(\"Y\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "        # Reconstructed images (Y_pred)\n",
        "        ax = plt.subplot(3, num_images, i + 1 + 2 * num_images)\n",
        "        plt.imshow(Y_pred[idx].reshape(50, 180), cmap=\"gray\")\n",
        "        plt.title(\"Reconstructed\")\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "u6kedDbxrcfq",
        "outputId": "13b85c16-e244-4cdd-8d50-6b6c005885b0"
      },
      "outputs": [],
      "source": [
        "display_images(X_train, Y_train, Y_train_pred, num_images=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "JOnfgRzBoXdd",
        "outputId": "658f505e-89e9-4822-8a1d-b558dbc72248"
      },
      "outputs": [],
      "source": [
        "display_images(X_val, Y_val, Y_val_pred, num_images=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
