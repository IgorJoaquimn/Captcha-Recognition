{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxI4LLcnZqRN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SY3XUmVsqEM1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 16:07:35.638095: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Input,Flatten,Reshape,AveragePooling2D,Dropout,LayerNormalization, ReLU,concatenate,Cropping2D, BatchNormalization\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input,Dropout, ReLU,BatchNormalization,Concatenate,LeakyReLU,Identity\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "font_path = '../dados/targa/Targa.ttf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVhNVxxbZ6qj",
        "outputId": "1ac5b4e0-1aad-4cd1-f1a3-70c23c374ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "unzip:  cannot find or open dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius, dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius.zip or dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ],
      "source": [
        "!wget  -nc https://www.dropbox.com/scl/fi/uaiyxp0t2l8hfcszfadtj/dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius&dl=1\n",
        "!unzip -n -q dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-owKNFD5ah24",
        "outputId": "adecb30f-3918-47dd-f02d-b8c94163f530"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jpg_file</th>\n",
              "      <th>txt_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000001.jpg</td>\n",
              "      <td>RNINIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000002.jpg</td>\n",
              "      <td>TVCFS8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000003.jpg</td>\n",
              "      <td>N1O1EH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000004.jpg</td>\n",
              "      <td>OQZSL4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000005.jpg</td>\n",
              "      <td>GST2YA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     jpg_file txt_content\n",
              "0  000001.jpg      RNINIC\n",
              "1  000002.jpg      TVCFS8\n",
              "2  000003.jpg      N1O1EH\n",
              "3  000004.jpg      OQZSL4\n",
              "4  000005.jpg      GST2YA"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = '../dados/CAPTCHA-10k/treinamento'\n",
        "def generate_df(image_path):\n",
        "  label_path = '../dados/CAPTCHA-10k/labels10k'\n",
        "\n",
        "  jpg_files = [f for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
        "  jpg_files.sort()\n",
        "  data = []\n",
        "\n",
        "  for jpg_file in jpg_files:\n",
        "      txt_file = os.path.splitext(jpg_file)[0] + '.txt'\n",
        "      txt_file_path = os.path.join(label_path, txt_file)\n",
        "\n",
        "      if os.path.exists(txt_file_path):\n",
        "          with open(txt_file_path, 'r') as file:\n",
        "              txt_content = file.read().strip()\n",
        "\n",
        "          data.append({'jpg_file': jpg_file, 'txt_content': txt_content})\n",
        "  return pd.DataFrame(data)\n",
        "\n",
        "df = generate_df(image_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'A', 'B',\n",
              "       'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O',\n",
              "       'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ']'],\n",
              "      dtype='<U1')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = np.unique(list(df['txt_content'].sum()) + [\"]\"])\n",
        "np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    max_pixel = 1.0\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=max_pixel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 16:07:38.136455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13917 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "autoencoder = tf.keras.models.load_model('model_BCE_aug_best_unet.tf',custom_objects={\"rmse\": rmse,\"psnr\":psnr})\n",
        "classifier = tf.keras.models.load_model('classifier_pre_trained.tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess(img):\n",
        "  kernel  = cv2.getStructuringElement(cv2.MORPH_RECT, (4, 4))\n",
        "  img     = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
        "  _, img  = cv2.threshold(img, 90, 255, cv2.THRESH_BINARY)\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 16:07:44.382004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
            "2024-08-02 16:07:45.197760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "247/247 [==============================] - 9s 27ms/step\n"
          ]
        }
      ],
      "source": [
        "def generate_X_Y(image_path):\n",
        "  df = generate_df(image_path)\n",
        "  df['has_question_mark'] = df['txt_content'].str.contains(r'\\?')\n",
        "  df = df[df['has_question_mark'] == False]\n",
        "  \n",
        "  df['txt_content'] = df['txt_content'].apply(lambda x: x + \"]\" if len(x) == 6 else x)\n",
        "\n",
        "  num_classes = len(vocab)\n",
        "  char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "  images = [preprocess(cv2.imread(os.path.join(image_path, x),cv2.IMREAD_GRAYSCALE)) for x in df[\"jpg_file\"]]\n",
        "  images = np.array(images)\n",
        "  images = np.expand_dims(images, axis=-1)\n",
        "  images = images.astype('float32') / 255.\n",
        "  images = autoencoder.predict(images)\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for i,text in enumerate(df['txt_content']):\n",
        "    x = images[i]    \n",
        "    interval = [0, 25, 50, 75, 100, 125, 150, 175]  # Adjusted intervals for 7 parts\n",
        "    for i in range(len(interval)-1):\n",
        "        fake_img = x[:,interval[i]:interval[i+1]]\n",
        "        y = np.zeros((1,num_classes))\n",
        "        y[0,char_to_index[text[i]]] = 1\n",
        "        X.append(fake_img)\n",
        "        Y.append(y)\n",
        "\n",
        "  X = np.array(X)\n",
        "  Y = np.array(Y)\n",
        "\n",
        "  X = X.reshape(-1,50,25,1)\n",
        "  Y = Y.reshape(-1,num_classes)\n",
        "  return X,Y,images\n",
        "\n",
        "X,Y,images = generate_X_Y('../dados/CAPTCHA-10k/treinamento')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x300 with 7 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 7, figsize=(20, 3))\n",
        "for i in range(7):\n",
        "    axes[i].imshow(X[i].reshape(50, 25), cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X)\n",
        "batch_size = 64\n",
        "augmented_data_generator = datagen.flow(X, Y, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StopTrainingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('accuracy') == 1.0:\n",
        "            print(\"\\nReached 100% accuracy, stopping training!\")\n",
        "            self.model.stop_training = True\n",
        "        \n",
        "        if logs.get('val_accuracy') == 1.0:\n",
        "            print(\"\\nReached 100% accuracy on val, stopping training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'classifier_full_trained_best.tf',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 16:07:55.013991: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x70aa6c03cc30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-08-02 16:07:55.014025: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
            "2024-08-02 16:07:55.018768: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-08-02 16:07:55.134490: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "861/864 [============================>.] - ETA: 0s - loss: 0.0665 - accuracy: 0.9869\n",
            "Epoch 1: val_accuracy improved from -inf to 0.98818, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 22s 21ms/step - loss: 0.0663 - accuracy: 0.9870 - val_loss: 0.0581 - val_accuracy: 0.9882\n",
            "Epoch 2/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0531 - accuracy: 0.9886\n",
            "Epoch 2: val_accuracy improved from 0.98818 to 0.98892, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 21ms/step - loss: 0.0530 - accuracy: 0.9887 - val_loss: 0.0580 - val_accuracy: 0.9889\n",
            "Epoch 3/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0496 - accuracy: 0.9888\n",
            "Epoch 3: val_accuracy improved from 0.98892 to 0.98973, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 21ms/step - loss: 0.0495 - accuracy: 0.9888 - val_loss: 0.0460 - val_accuracy: 0.9897\n",
            "Epoch 4/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9890\n",
            "Epoch 4: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0481 - accuracy: 0.9891 - val_loss: 0.0576 - val_accuracy: 0.9879\n",
            "Epoch 5/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0455 - accuracy: 0.9889\n",
            "Epoch 5: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0454 - accuracy: 0.9889 - val_loss: 0.0483 - val_accuracy: 0.9896\n",
            "Epoch 6/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9895\n",
            "Epoch 6: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0458 - accuracy: 0.9895 - val_loss: 0.0473 - val_accuracy: 0.9897\n",
            "Epoch 7/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9894\n",
            "Epoch 7: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0439 - accuracy: 0.9894 - val_loss: 0.1538 - val_accuracy: 0.9435\n",
            "Epoch 8/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0418 - accuracy: 0.9900\n",
            "Epoch 8: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0418 - accuracy: 0.9900 - val_loss: 0.0509 - val_accuracy: 0.9896\n",
            "Epoch 9/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0415 - accuracy: 0.9896\n",
            "Epoch 9: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0415 - accuracy: 0.9896 - val_loss: 1.5637 - val_accuracy: 0.5044\n",
            "Epoch 10/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.9897\n",
            "Epoch 10: val_accuracy did not improve from 0.98973\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0417 - accuracy: 0.9897 - val_loss: 0.4308 - val_accuracy: 0.8544\n",
            "Epoch 11/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9902\n",
            "Epoch 11: val_accuracy improved from 0.98973 to 0.99042, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 20ms/step - loss: 0.0400 - accuracy: 0.9902 - val_loss: 0.0422 - val_accuracy: 0.9904\n",
            "Epoch 12/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9903\n",
            "Epoch 12: val_accuracy did not improve from 0.99042\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0391 - accuracy: 0.9903 - val_loss: 0.0957 - val_accuracy: 0.9682\n",
            "Epoch 13/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9904\n",
            "Epoch 13: val_accuracy did not improve from 0.99042\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0393 - accuracy: 0.9904 - val_loss: 0.0553 - val_accuracy: 0.9857\n",
            "Epoch 14/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0377 - accuracy: 0.9903\n",
            "Epoch 14: val_accuracy did not improve from 0.99042\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0377 - accuracy: 0.9903 - val_loss: 0.3351 - val_accuracy: 0.8625\n",
            "Epoch 15/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9900\n",
            "Epoch 15: val_accuracy did not improve from 0.99042\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 0.0608 - val_accuracy: 0.9865\n",
            "Epoch 16/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0370 - accuracy: 0.9905\n",
            "Epoch 16: val_accuracy did not improve from 0.99042\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0372 - accuracy: 0.9905 - val_loss: 0.1026 - val_accuracy: 0.9578\n",
            "Epoch 17/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9902\n",
            "Epoch 17: val_accuracy did not improve from 0.99042\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0379 - accuracy: 0.9902 - val_loss: 0.0752 - val_accuracy: 0.9701\n",
            "Epoch 18/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9908\n",
            "Epoch 18: val_accuracy improved from 0.99042 to 0.99064, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 21ms/step - loss: 0.0368 - accuracy: 0.9908 - val_loss: 0.0356 - val_accuracy: 0.9906\n",
            "Epoch 19/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9908\n",
            "Epoch 19: val_accuracy did not improve from 0.99064\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0358 - accuracy: 0.9908 - val_loss: 0.0388 - val_accuracy: 0.9901\n",
            "Epoch 20/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0357 - accuracy: 0.9906\n",
            "Epoch 20: val_accuracy improved from 0.99064 to 0.99138, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 17s 20ms/step - loss: 0.0357 - accuracy: 0.9906 - val_loss: 0.0327 - val_accuracy: 0.9914\n",
            "Epoch 21/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0351 - accuracy: 0.9906\n",
            "Epoch 21: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0352 - accuracy: 0.9906 - val_loss: 1.0687 - val_accuracy: 0.6747\n",
            "Epoch 22/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0358 - accuracy: 0.9908\n",
            "Epoch 22: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0357 - accuracy: 0.9908 - val_loss: 0.0384 - val_accuracy: 0.9908\n",
            "Epoch 23/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9910\n",
            "Epoch 23: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0342 - accuracy: 0.9910 - val_loss: 0.3172 - val_accuracy: 0.8899\n",
            "Epoch 24/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0344 - accuracy: 0.9906\n",
            "Epoch 24: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0344 - accuracy: 0.9906 - val_loss: 0.0691 - val_accuracy: 0.9866\n",
            "Epoch 25/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9913\n",
            "Epoch 25: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0336 - accuracy: 0.9913 - val_loss: 0.0532 - val_accuracy: 0.9897\n",
            "Epoch 26/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9911\n",
            "Epoch 26: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0331 - accuracy: 0.9911 - val_loss: 1.1161 - val_accuracy: 0.7800\n",
            "Epoch 27/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9915\n",
            "Epoch 27: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0327 - accuracy: 0.9915 - val_loss: 0.0382 - val_accuracy: 0.9905\n",
            "Epoch 28/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9911\n",
            "Epoch 28: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0329 - accuracy: 0.9911 - val_loss: 0.0662 - val_accuracy: 0.9816\n",
            "Epoch 29/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9912\n",
            "Epoch 29: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0329 - accuracy: 0.9912 - val_loss: 0.0876 - val_accuracy: 0.9655\n",
            "Epoch 30/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0318 - accuracy: 0.9914\n",
            "Epoch 30: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0318 - accuracy: 0.9915 - val_loss: 0.3548 - val_accuracy: 0.8841\n",
            "Epoch 31/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0329 - accuracy: 0.9913\n",
            "Epoch 31: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0328 - accuracy: 0.9913 - val_loss: 0.0460 - val_accuracy: 0.9905\n",
            "Epoch 32/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9909\n",
            "Epoch 32: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0321 - accuracy: 0.9910 - val_loss: 0.0339 - val_accuracy: 0.9913\n",
            "Epoch 33/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0317 - accuracy: 0.9915\n",
            "Epoch 33: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0318 - accuracy: 0.9915 - val_loss: 0.1991 - val_accuracy: 0.9558\n",
            "Epoch 34/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0310 - accuracy: 0.9917\n",
            "Epoch 34: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0310 - accuracy: 0.9916 - val_loss: 0.0745 - val_accuracy: 0.9838\n",
            "Epoch 35/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0322 - accuracy: 0.9910\n",
            "Epoch 35: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0322 - accuracy: 0.9911 - val_loss: 0.0761 - val_accuracy: 0.9843\n",
            "Epoch 36/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9912\n",
            "Epoch 36: val_accuracy did not improve from 0.99138\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0315 - accuracy: 0.9912 - val_loss: 0.0328 - val_accuracy: 0.9912\n",
            "Epoch 37/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0309 - accuracy: 0.9913\n",
            "Epoch 37: val_accuracy improved from 0.99138 to 0.99145, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 17s 20ms/step - loss: 0.0308 - accuracy: 0.9913 - val_loss: 0.0388 - val_accuracy: 0.9914\n",
            "Epoch 38/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0303 - accuracy: 0.9916\n",
            "Epoch 38: val_accuracy did not improve from 0.99145\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0305 - accuracy: 0.9915 - val_loss: 0.3811 - val_accuracy: 0.9061\n",
            "Epoch 39/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9914\n",
            "Epoch 39: val_accuracy did not improve from 0.99145\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0306 - accuracy: 0.9914 - val_loss: 0.6006 - val_accuracy: 0.7920\n",
            "Epoch 40/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9917\n",
            "Epoch 40: val_accuracy improved from 0.99145 to 0.99167, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 20ms/step - loss: 0.0301 - accuracy: 0.9917 - val_loss: 0.0332 - val_accuracy: 0.9917\n",
            "Epoch 41/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9918\n",
            "Epoch 41: val_accuracy improved from 0.99167 to 0.99176, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 20ms/step - loss: 0.0298 - accuracy: 0.9918 - val_loss: 0.0344 - val_accuracy: 0.9918\n",
            "Epoch 42/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0296 - accuracy: 0.9916\n",
            "Epoch 42: val_accuracy did not improve from 0.99176\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0296 - accuracy: 0.9916 - val_loss: 0.0326 - val_accuracy: 0.9914\n",
            "Epoch 43/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9917\n",
            "Epoch 43: val_accuracy did not improve from 0.99176\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0298 - accuracy: 0.9917 - val_loss: 0.0375 - val_accuracy: 0.9915\n",
            "Epoch 44/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9917\n",
            "Epoch 44: val_accuracy improved from 0.99176 to 0.99186, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 17s 20ms/step - loss: 0.0299 - accuracy: 0.9917 - val_loss: 0.0321 - val_accuracy: 0.9919\n",
            "Epoch 45/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0289 - accuracy: 0.9921\n",
            "Epoch 45: val_accuracy did not improve from 0.99186\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0290 - accuracy: 0.9921 - val_loss: 0.0452 - val_accuracy: 0.9905\n",
            "Epoch 46/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0298 - accuracy: 0.9915\n",
            "Epoch 46: val_accuracy did not improve from 0.99186\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0298 - accuracy: 0.9915 - val_loss: 0.0374 - val_accuracy: 0.9915\n",
            "Epoch 47/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0290 - accuracy: 0.9919\n",
            "Epoch 47: val_accuracy improved from 0.99186 to 0.99226, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 21ms/step - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.0304 - val_accuracy: 0.9923\n",
            "Epoch 48/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9917\n",
            "Epoch 48: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0292 - accuracy: 0.9917 - val_loss: 0.0432 - val_accuracy: 0.9907\n",
            "Epoch 49/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9920\n",
            "Epoch 49: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0284 - accuracy: 0.9920 - val_loss: 0.8192 - val_accuracy: 0.8689\n",
            "Epoch 50/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0290 - accuracy: 0.9918\n",
            "Epoch 50: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0289 - accuracy: 0.9919 - val_loss: 0.0844 - val_accuracy: 0.9800\n",
            "Epoch 51/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9920\n",
            "Epoch 51: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0283 - accuracy: 0.9920 - val_loss: 0.0305 - val_accuracy: 0.9908\n",
            "Epoch 52/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9923\n",
            "Epoch 52: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0284 - accuracy: 0.9923 - val_loss: 0.0747 - val_accuracy: 0.9810\n",
            "Epoch 53/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9917\n",
            "Epoch 53: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0279 - accuracy: 0.9917 - val_loss: 0.0378 - val_accuracy: 0.9904\n",
            "Epoch 54/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0284 - accuracy: 0.9920\n",
            "Epoch 54: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0284 - accuracy: 0.9920 - val_loss: 0.0453 - val_accuracy: 0.9887\n",
            "Epoch 55/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0285 - accuracy: 0.9920\n",
            "Epoch 55: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0284 - accuracy: 0.9920 - val_loss: 0.0429 - val_accuracy: 0.9857\n",
            "Epoch 56/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0279 - accuracy: 0.9920\n",
            "Epoch 56: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0279 - accuracy: 0.9919 - val_loss: 0.0372 - val_accuracy: 0.9909\n",
            "Epoch 57/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9918\n",
            "Epoch 57: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0281 - accuracy: 0.9918 - val_loss: 0.1145 - val_accuracy: 0.9522\n",
            "Epoch 58/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9923\n",
            "Epoch 58: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0275 - accuracy: 0.9922 - val_loss: 0.0349 - val_accuracy: 0.9916\n",
            "Epoch 59/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9916\n",
            "Epoch 59: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0280 - accuracy: 0.9916 - val_loss: 0.0471 - val_accuracy: 0.9858\n",
            "Epoch 60/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0273 - accuracy: 0.9922\n",
            "Epoch 60: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0272 - accuracy: 0.9922 - val_loss: 0.0406 - val_accuracy: 0.9916\n",
            "Epoch 61/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9923\n",
            "Epoch 61: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0276 - accuracy: 0.9923 - val_loss: 0.0273 - val_accuracy: 0.9923\n",
            "Epoch 62/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9919\n",
            "Epoch 62: val_accuracy did not improve from 0.99226\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0271 - accuracy: 0.9919 - val_loss: 0.0334 - val_accuracy: 0.9914\n",
            "Epoch 63/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9920\n",
            "Epoch 63: val_accuracy improved from 0.99226 to 0.99237, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 17s 20ms/step - loss: 0.0272 - accuracy: 0.9920 - val_loss: 0.0273 - val_accuracy: 0.9924\n",
            "Epoch 64/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0274 - accuracy: 0.9920\n",
            "Epoch 64: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0275 - accuracy: 0.9920 - val_loss: 0.0378 - val_accuracy: 0.9914\n",
            "Epoch 65/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0270 - accuracy: 0.9923\n",
            "Epoch 65: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0271 - accuracy: 0.9923 - val_loss: 0.0326 - val_accuracy: 0.9907\n",
            "Epoch 66/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9923\n",
            "Epoch 66: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0261 - accuracy: 0.9923 - val_loss: 0.0286 - val_accuracy: 0.9923\n",
            "Epoch 67/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9921\n",
            "Epoch 67: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0268 - accuracy: 0.9921 - val_loss: 1.7817 - val_accuracy: 0.6999\n",
            "Epoch 68/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9924\n",
            "Epoch 68: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0267 - accuracy: 0.9924 - val_loss: 0.0307 - val_accuracy: 0.9920\n",
            "Epoch 69/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9924\n",
            "Epoch 69: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0263 - accuracy: 0.9924 - val_loss: 0.0317 - val_accuracy: 0.9914\n",
            "Epoch 70/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9922\n",
            "Epoch 70: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 6.1054 - val_accuracy: 0.2618\n",
            "Epoch 71/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9920\n",
            "Epoch 71: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0266 - accuracy: 0.9920 - val_loss: 0.1876 - val_accuracy: 0.9599\n",
            "Epoch 72/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9927\n",
            "Epoch 72: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0263 - accuracy: 0.9926 - val_loss: 0.0592 - val_accuracy: 0.9870\n",
            "Epoch 73/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9921\n",
            "Epoch 73: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0261 - accuracy: 0.9921 - val_loss: 0.1709 - val_accuracy: 0.9091\n",
            "Epoch 74/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9924\n",
            "Epoch 74: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0263 - accuracy: 0.9924 - val_loss: 0.5672 - val_accuracy: 0.8730\n",
            "Epoch 75/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9925\n",
            "Epoch 75: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0256 - accuracy: 0.9925 - val_loss: 0.0293 - val_accuracy: 0.9921\n",
            "Epoch 76/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9923\n",
            "Epoch 76: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0260 - accuracy: 0.9923 - val_loss: 0.0509 - val_accuracy: 0.9904\n",
            "Epoch 77/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9922\n",
            "Epoch 77: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0255 - accuracy: 0.9922 - val_loss: 0.0465 - val_accuracy: 0.9896\n",
            "Epoch 78/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9924\n",
            "Epoch 78: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0259 - accuracy: 0.9924 - val_loss: 0.0965 - val_accuracy: 0.9690\n",
            "Epoch 79/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0261 - accuracy: 0.9924\n",
            "Epoch 79: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.0367 - val_accuracy: 0.9900\n",
            "Epoch 80/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9926\n",
            "Epoch 80: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0255 - accuracy: 0.9926 - val_loss: 0.0341 - val_accuracy: 0.9921\n",
            "Epoch 81/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0261 - accuracy: 0.9924\n",
            "Epoch 81: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0261 - accuracy: 0.9924 - val_loss: 0.0493 - val_accuracy: 0.9900\n",
            "Epoch 82/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0254 - accuracy: 0.9923\n",
            "Epoch 82: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0253 - accuracy: 0.9923 - val_loss: 0.0279 - val_accuracy: 0.9920\n",
            "Epoch 83/500\n",
            "863/864 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.9928\n",
            "Epoch 83: val_accuracy did not improve from 0.99237\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0247 - accuracy: 0.9928 - val_loss: 0.0477 - val_accuracy: 0.9902\n",
            "Epoch 84/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9925\n",
            "Epoch 84: val_accuracy improved from 0.99237 to 0.99268, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 18s 21ms/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 0.0252 - val_accuracy: 0.9927\n",
            "Epoch 85/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9926\n",
            "Epoch 85: val_accuracy did not improve from 0.99268\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0252 - accuracy: 0.9926 - val_loss: 0.0579 - val_accuracy: 0.9839\n",
            "Epoch 86/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9923\n",
            "Epoch 86: val_accuracy did not improve from 0.99268\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0251 - accuracy: 0.9924 - val_loss: 0.0284 - val_accuracy: 0.9926\n",
            "Epoch 87/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0255 - accuracy: 0.9927\n",
            "Epoch 87: val_accuracy did not improve from 0.99268\n",
            "864/864 [==============================] - 16s 18ms/step - loss: 0.0255 - accuracy: 0.9927 - val_loss: 0.1279 - val_accuracy: 0.9619\n",
            "Epoch 88/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9924\n",
            "Epoch 88: val_accuracy did not improve from 0.99268\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0252 - accuracy: 0.9924 - val_loss: 0.0300 - val_accuracy: 0.9923\n",
            "Epoch 89/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9924\n",
            "Epoch 89: val_accuracy improved from 0.99268 to 0.99293, saving model to classifier_full_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "864/864 [==============================] - 17s 20ms/step - loss: 0.0250 - accuracy: 0.9924 - val_loss: 0.0284 - val_accuracy: 0.9929\n",
            "Epoch 90/500\n",
            "864/864 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9927\n",
            "Epoch 90: val_accuracy did not improve from 0.99293\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0250 - accuracy: 0.9927 - val_loss: 0.0450 - val_accuracy: 0.9887\n",
            "Epoch 91/500\n",
            "862/864 [============================>.] - ETA: 0s - loss: 0.0252 - accuracy: 0.9924\n",
            "Epoch 91: val_accuracy did not improve from 0.99293\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0253 - accuracy: 0.9924 - val_loss: 0.0266 - val_accuracy: 0.9925\n",
            "Epoch 92/500\n",
            "861/864 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9927\n",
            "Epoch 92: val_accuracy did not improve from 0.99293\n",
            "864/864 [==============================] - 16s 19ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.0372 - val_accuracy: 0.9912\n",
            "Epoch 93/500\n",
            "624/864 [====================>.........] - ETA: 3s - loss: 0.0236 - accuracy: 0.9929"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m stop_training_callback \u001b[38;5;241m=\u001b[39m StopTrainingCallback()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_data_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_training_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "stop_training_callback = StopTrainingCallback()\n",
        "classifier.fit(augmented_data_generator,steps_per_epoch=len(X) // batch_size, epochs=500, \n",
        "               callbacks=[stop_training_callback,checkpoint],\n",
        "               validation_data=(X, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_full_trained.tf/assets\n"
          ]
        }
      ],
      "source": [
        "classifier.save(\"classifier_full_trained.tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1729/1729 [==============================] - 5s 3ms/step - loss: 1.7161 - accuracy: 0.6909\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.716137409210205, 0.6909281611442566]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.evaluate(X,Y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
