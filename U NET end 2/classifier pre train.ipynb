{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gxI4LLcnZqRN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SY3XUmVsqEM1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 13:04:52.858211: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, UpSampling2D, Input,Flatten,Reshape,AveragePooling2D,Dropout,LayerNormalization, ReLU,concatenate,Cropping2D, BatchNormalization\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Input,Dropout, ReLU,BatchNormalization,Concatenate,LeakyReLU,Identity\n",
        "import tensorflow as tf\n",
        "\n",
        "import keras\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "font_path = '../dados/targa/Targa.ttf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVhNVxxbZ6qj",
        "outputId": "1ac5b4e0-1aad-4cd1-f1a3-70c23c374ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: /home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "/bin/bash: /home/igu/miniconda3/envs/ml/lib/python3.9/site-packages/cv2/../../../../lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
            "unzip:  cannot find or open dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius, dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius.zip or dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius.ZIP.\n",
            "\n",
            "No zipfiles found.\n"
          ]
        }
      ],
      "source": [
        "!wget  -nc https://www.dropbox.com/scl/fi/uaiyxp0t2l8hfcszfadtj/dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius&dl=1\n",
        "!unzip -n -q dados.zip?rlkey=lnqcb79vbu8j6cdbfgofogius"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-owKNFD5ah24",
        "outputId": "adecb30f-3918-47dd-f02d-b8c94163f530"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jpg_file</th>\n",
              "      <th>txt_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000001.jpg</td>\n",
              "      <td>RNINIC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000002.jpg</td>\n",
              "      <td>TVCFS8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000003.jpg</td>\n",
              "      <td>N1O1EH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000004.jpg</td>\n",
              "      <td>OQZSL4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000005.jpg</td>\n",
              "      <td>GST2YA</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     jpg_file txt_content\n",
              "0  000001.jpg      RNINIC\n",
              "1  000002.jpg      TVCFS8\n",
              "2  000003.jpg      N1O1EH\n",
              "3  000004.jpg      OQZSL4\n",
              "4  000005.jpg      GST2YA"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_path = '../dados/CAPTCHA-10k/treinamento'\n",
        "def generate_df(image_path):\n",
        "  label_path = '../dados/CAPTCHA-10k/labels10k'\n",
        "\n",
        "  jpg_files = [f for f in os.listdir(image_path) if f.endswith('.jpg')]\n",
        "  jpg_files.sort()\n",
        "  data = []\n",
        "\n",
        "  for jpg_file in jpg_files:\n",
        "      txt_file = os.path.splitext(jpg_file)[0] + '.txt'\n",
        "      txt_file_path = os.path.join(label_path, txt_file)\n",
        "\n",
        "      if os.path.exists(txt_file_path):\n",
        "          with open(txt_file_path, 'r') as file:\n",
        "              txt_content = file.read().strip()\n",
        "\n",
        "          data.append({'jpg_file': jpg_file, 'txt_content': txt_content})\n",
        "  return pd.DataFrame(data)\n",
        "\n",
        "df = generate_df(image_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', 'A', 'B',\n",
              "       'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O',\n",
              "       'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ']'],\n",
              "      dtype='<U1')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = np.unique(list(df['txt_content'].sum()) + [\"]\"])\n",
        "np.array(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NGKSD3tFpvIH"
      },
      "outputs": [],
      "source": [
        "def generate_clean_captcha(text):\n",
        "    # Fixed parameters\n",
        "    size = (180, 50)  # Change size to (height, width)\n",
        "    font_size = 24\n",
        "    num_parts = 7\n",
        "\n",
        "    # Create a blank white image\n",
        "    image = Image.new('L', size, 255)  # 'L' mode for grayscale\n",
        "\n",
        "    # Load the custom font\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "    # Create a drawing context\n",
        "    draw = ImageDraw.Draw(image)\n",
        "\n",
        "    # Calculate positions for each part\n",
        "    part_width = size[0] / num_parts\n",
        "    horizontal_positions = [int(part_width * i + part_width / 2) for i in range(num_parts)]\n",
        "    horizontal_positions = horizontal_positions[:len(text)]  # Adjust to the length of the text\n",
        "\n",
        "    # Calculate y position to center the text vertically\n",
        "    text_bbox = draw.textbbox((0, 0), text, font=font)\n",
        "    text_height = text_bbox[3] - text_bbox[1]\n",
        "    text_y = (size[1] - text_height) // 2\n",
        "\n",
        "    # Draw each letter at the calculated position\n",
        "    for char, x in zip(text, horizontal_positions):\n",
        "        char_bbox = draw.textbbox((0, 0), char, font=font)\n",
        "        char_width = char_bbox[2] - char_bbox[0]\n",
        "        char_x = x - char_width // 2  # Center the character horizontally within its part\n",
        "        draw.text((char_x, text_y), char, font=font, fill=0)\n",
        "\n",
        "    # Convert to numpy array if needed for further processing with OpenCV\n",
        "    captcha_image = np.array(image)\n",
        "\n",
        "    return captcha_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_X_Y(image_path):\n",
        "  df = generate_df(image_path)\n",
        "  df['txt_content'] = df['txt_content'].apply(lambda x: x + \"]\" if len(x) == 6 else x)\n",
        "\n",
        "  num_classes = len(vocab)\n",
        "  char_to_index = {char: idx for idx, char in enumerate(vocab)}\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for text in df['txt_content']:\n",
        "    x = generate_clean_captcha(text)\n",
        "    interval = [0, 25, 50, 75, 100, 125, 150, 175]  # Adjusted intervals for 7 parts\n",
        "    for i in range(len(interval)-1):\n",
        "        fake_img = x[:,interval[i]:interval[i+1]]\n",
        "        y = np.zeros((1,num_classes))\n",
        "        y[0,char_to_index[text[i]]] = 1\n",
        "        X.append(fake_img)\n",
        "        Y.append(y)\n",
        "\n",
        "  X = np.array(X)\n",
        "  Y = np.array(Y)\n",
        "\n",
        "  X = (X.astype('float32'))  / 255.0\n",
        "\n",
        "  X = X.reshape(-1,50,25,1)\n",
        "  Y = Y.reshape(-1,num_classes)\n",
        "  return X,Y\n",
        "\n",
        "X,Y = generate_X_Y('../dados/CAPTCHA-10k/treinamento')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x300 with 7 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, axes = plt.subplots(1, 7, figsize=(20, 3))\n",
        "for i in range(7):\n",
        "    axes[i].imshow(X[i].reshape(50, 25), cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.05,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=False,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "datagen.fit(X)\n",
        "batch_size = 8\n",
        "augmented_data_generator = datagen.flow(X, Y, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Camada Convolution-BatchNorm-ReLu\n",
        "def CK(filters, kernel_size=(3, 3), strides=(1, 1), padding='same', use_batch_norm=True, downsample=True):\n",
        "    '''\n",
        "        filters: quantidade de filtros\n",
        "        kernel_size 3x3 | strides 1x1 | padding same | sao constantes durante o codigo\n",
        "        use_batch_norm ->   indica quando devemos usar BatchNormalization, em caso de negativo, a camada se torna a Identidade\n",
        "        downsample ->       indica se a dimensao deve aumentar ou diminuir \n",
        "    '''\n",
        "\n",
        "    # Esse chavemento usando  if ternario serve para selecionar as camadas com base nos atributos\n",
        "    conv = Conv2D               if downsample       else Conv2DTranspose\n",
        "    norm = BatchNormalization   if use_batch_norm   else Identity\n",
        "    actf = LeakyReLU(0.2)       if downsample       else ReLU()\n",
        "    # alpha de 0.2 na LeakyReLU foi definido no paper original\n",
        "\n",
        "    # Com o chaveamento pronto, a camada pode ser montada sequencialmente\n",
        "    def layer(x):\n",
        "        x = conv(filters, kernel_size, strides=strides, padding=padding)(x)\n",
        "        x = norm()(x)\n",
        "        x = actf(x)\n",
        "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "        return x\n",
        "    return layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " target_image (InputLayer)   [(None, 50, 25, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 50, 25, 16)        160       \n",
            "                                                                 \n",
            " identity (Identity)         (None, 50, 25, 16)        0         \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 50, 25, 16)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 25, 13, 16)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 25, 13, 32)        4640      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 25, 13, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 25, 13, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 13, 7, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 13, 7, 64)         18496     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 13, 7, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 13, 7, 64)         0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 7, 4, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 7, 4, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 7, 4, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 7, 4, 128)         0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 4, 2, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 4, 2, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 4, 2, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 4, 2, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 2, 1, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 2, 1, 16)          4112      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 38)                1254      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 399606 (1.52 MB)\n",
            "Trainable params: 398646 (1.52 MB)\n",
            "Non-trainable params: 960 (3.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 13:04:59.079883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13856 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        }
      ],
      "source": [
        "def discriminator(output_nc, ngf, num_downsample=3):\n",
        "\n",
        "    # O discriminador por sua vez, recebe a entrada e a saida do modelo, tentando assim decidir se aquilo 'e real ou nao\n",
        "    tar = Input(shape=[50, 25, output_nc], name='target_image')\n",
        "\n",
        "    # Initial convolutional layers # SEM BATCH NORM !\n",
        "    x = CK(ngf,use_batch_norm=False)(tar)\n",
        "\n",
        "    # Contracting path\n",
        "    for i in range(num_downsample):\n",
        "        x = CK(ngf*(2**(i+1)))(x)\n",
        "    \n",
        "    # Por fim, a patchGan gera uma classificao binaria por patch, o tamanho do patch eh definido pelo num_downsample. Quanto maior, menor a area de recepcao\n",
        "    # Por exemplo num_downsample = 4 faz com que o discriminador classifique blocos de 16x16\n",
        "    x = Conv2D(ngf, (1, 1),activation='ReLU')(x)\n",
        "    x = Flatten()(x)\n",
        "    # x = Dense(64,activation=\"ReLU\")(x)\n",
        "    x = Dense(38,activation=\"softmax\")(x)\n",
        "    return Model(inputs=tar, outputs=x)\n",
        "\n",
        "classifier = discriminator(1,16,4)\n",
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class StopTrainingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('accuracy') == 1.0:\n",
        "            print(\"\\nReached 100% accuracy, stopping training!\")\n",
        "            self.model.stop_training = True\n",
        "        \n",
        "        if logs.get('val_accuracy') == 1.0:\n",
        "            print(\"\\nReached 100% accuracy on val, stopping training!\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    'classifier_pre_trained_best.tf',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-08-02 13:05:01.045343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
            "2024-08-02 13:05:01.518572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2024-08-02 13:05:01.559882: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fbeb851f9d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-08-02 13:05:01.560041: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Ti, Compute Capability 8.9\n",
            "2024-08-02 13:05:01.566296: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-08-02 13:05:01.700411: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6995/7000 [============================>.] - ETA: 0s - loss: 0.1523 - accuracy: 0.9598\n",
            "Reached 100% accuracy on val, stopping training!\n",
            "\n",
            "Epoch 1: val_accuracy improved from -inf to 1.00000, saving model to classifier_pre_trained_best.tf\n",
            "INFO:tensorflow:Assets written to: classifier_pre_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_pre_trained_best.tf/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7000/7000 [==============================] - 49s 6ms/step - loss: 0.1522 - accuracy: 0.9598 - val_loss: 6.9848e-06 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7fc0e32e9cd0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_training_callback = StopTrainingCallback()\n",
        "classifier.fit(augmented_data_generator,steps_per_epoch=len(X) // batch_size, epochs=500, \n",
        "               callbacks=[stop_training_callback,checkpoint],\n",
        "               validation_data=(X, Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_pre_trained.tf/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: classifier_pre_trained.tf/assets\n"
          ]
        }
      ],
      "source": [
        "classifier.save(\"classifier_pre_trained.tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1750/1750 [==============================] - 5s 3ms/step - loss: 6.9848e-06 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[6.984756964811822e-06, 1.0]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.evaluate(X,Y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
